FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=1.00000000000000005551e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=11 11 2 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 8.02942338730880278419e-01) (1, 4.55965215267062462523e-01) (2, 4.09064428807616509154e-01) (3, 3.65864777864337242796e-01) (4, 3.50902160556435860350e-01) (5, -1.20088198832119830151e+00) (6, -3.34435392352872984389e+00) (7, 5.09804983988766502456e-01) (8, -4.54843642106240064749e-01) (9, -1.80340666513922798231e+00) (10, 3.59409768021886288913e-01) (0, 3.24900535537256063279e-01) (1, 4.54931066916865245453e-01) (2, 6.05066099749487773529e-01) (3, 4.27122266577166342927e-01) (4, 5.93122744546812907807e-01) (5, -3.57549866351451273161e+00) (6, 2.55184602226853740614e-01) (7, 2.40293310933616366398e+00) (8, 3.04121039527826919624e+00) (9, -4.20731168514623021792e+00) (10, 6.80243332448838500248e-01) (0, -3.24018911677880050437e-01) (1, 9.01755775061037212481e-02) (2, 1.74745150908508561916e-01) (3, 2.37411932155170757319e-01) (4, 1.33260347947159157878e-01) (5, -2.27748990515058880035e+00) (6, 1.02616061242795075792e+00) (7, 5.89495686286923559205e-02) (8, 4.94714696731131109431e+00) (9, 2.86575268008267380893e+00) (10, 2.17439793925801483354e-01) (0, -3.03242487731077203783e-01) (1, 1.86787284372825601597e-01) (2, 6.41310060606194304311e-02) (3, 1.99134148119468723515e-01) (4, 2.10792786835212742069e-01) (5, -5.97184525544124378271e-01) (6, 9.28755017248989345191e-01) (7, 5.47385954719434731502e-03) (8, 7.11565121775641351576e-01) (9, 1.53409638527592950830e+00) (10, 1.95951525027560080616e-01) (0, 4.15781471332132501750e-01) (1, 2.69624619421772009076e-02) (2, 2.42778091368488317670e-02) (3, 4.53382386503032724923e-02) (4, -6.98073030772396047050e-02) (5, 1.90575017577303960081e+01) (6, -8.01572201219434066211e-01) (7, 1.11416974178148650565e-01) (8, -2.99566592990570468302e+00) (9, -2.64599887438697134456e+00) (10, 2.50096142550492069045e-02) (0, -7.69772716766531778232e-02) (1, 1.99872500227101096426e-01) (2, 2.02783203647740134512e-01) (3, 1.81571296380169638907e-01) (4, 1.70752680824406394278e-01) (5, -7.55183627651030509043e+00) (6, 1.69819610652464159095e+00) (7, 2.14190326747135095786e+00) (8, 3.33015806087262618007e+01) (9, 1.35378536508604749855e+01) (10, 2.93657344669509934310e-01) (0, -3.49653026346066986285e-01) (1, -1.29159839250010871625e-01) (2, -5.43453929795450069173e-02) (3, -2.68836555852121281140e-02) (4, -1.13068418003482218381e-01) (5, -1.26171548137912981069e+00) (6, -6.52924521339701691147e-01) (7, -8.86461960427317841038e-01) (8, -4.91836035112681102532e+00) (9, 1.44859330878016523414e+00) (10, -1.26025995996294826185e-01) (0, -7.19125555517184378296e-02) (1, -5.26645092666912437163e-02) (2, -1.22029690295534162803e-01) (3, -2.16267293542699967634e-01) (4, -2.12906239777879868758e-01) (5, 1.51248972179961360673e+00) (6, 1.40998704496090560001e-01) (7, -3.00364957079208461099e-01) (8, 6.49607626079158784194e-01) (9, 4.80161771712616591845e-01) (10, -3.12231405234185355191e-01) (0, -2.31040489018553008016e-01) (1, 2.15085804990066492559e-01) (2, 1.40917577377094288549e-01) (3, 2.37363562515510578832e-01) (4, 1.81015506914390528204e-01) (5, -7.62624634251887290048e+00) (6, 9.18285761988315463178e-01) (7, 2.08820521479224829875e+00) (8, 3.35508870936922392048e+01) (9, 1.34846707600981847719e+01) (10, 2.33118173569847070281e-01) (0, -3.72145272372571278563e-01) (1, -1.75027618804987877832e-01) (2, -2.62253033617552699930e-01) (3, -1.92994716266211480127e-01) (4, -1.50457399586256951318e-01) (5, 1.92454710282537067023e+00) (6, 4.47057477949834414233e-01) (7, -5.63739054451619869823e-01) (8, 8.16356071566223140401e-01) (9, 1.00256566698217342015e+00) (10, -2.61430092254250900918e-01) (11, -6.97157258130806922747e-02) (12, -8.83984660102973829332e-02) (13, 3.10903248237401175214e-01) (14, 3.13585725473195242596e-01) (15, 7.02914034936205966986e-01) (16, 3.17157358636594055579e-01) (17, 1.48174611242524512100e-01) (18, 7.97222732204766804109e-01) (19, 2.70799145807957941656e-01) (20, 7.86707556444974476229e-01) (21, 3.33368600485551946910e-01) 
